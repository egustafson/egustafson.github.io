<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Quintessential Ceph | Stuff I&#39;ve Figured Out</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
        <link rel="stylesheet" href="/css/style_custom.css" />

  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">Home</a></li>
      
      <li><a href="/about/">About</a></li>
      
      <li><a href="/tags/">Tags</a></li>
      
      <li><a href="/credits/">Credits</a></li>
      
      <li><a href="/index.xml">Subscribe</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">Quintessential Ceph</span></h1>
<h2 class="author">Eric Gustafson</h2>
<h2 class="date">2016/04/20</h2>
</div>

<main>


<p>The following is my distillation of how to install <a href="http://docs.ceph.com">Ceph</a> in the most minimal
way.  This method of installation is <em>NOT</em> the {right, correct, best, &hellip;} way
to install Ceph.  It is what I could discern as the most minimal way to install
the smallest set of Ceph components to have a minimally running system.   I did
this to better understand Ceph and hope it will help others as well.</p>

<h1 id="architecture-what-s-needed">Architecture &amp; what&rsquo;s needed</h1>

<p>The <a href="http://docs.ceph.com/docs/master/architecture/">Ceph Architecture</a> page
starts with both a good diagram and description of what makes up the cluster.</p>

<h1 id="minimal-steps-to-create-a-ceph-cluster">Minimal Steps to create a Ceph cluster</h1>

<p>The architecture:  4 Hosts</p>

<ul>
<li>3x Storage Nodes  (have second &ldquo;storage&rdquo; drive)

<ul>
<li>hostnames:  ceph-osd{0,1,2}</li>
</ul></li>
<li>1x Monitor Node

<ul>
<li>hostname:  ceph-mon0</li>
</ul></li>
</ul>

<h2 id="prerequisites">Prerequisites</h2>

<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>

<tbody>
<tr>
<td>Hosts:</td>
<td>4 &ndash; ok, this isn&rsquo;t the minimum, but for this example it is.</td>
</tr>

<tr>
<td>OS:</td>
<td>Ubuntu (4G root is sufficient) // I used Wily (15.10) Server</td>
</tr>

<tr>
<td>OSD Disk:</td>
<td>A second &ldquo;disk&rdquo; for the OSD hosts.  (sdb)</td>
</tr>

<tr>
<td>DNS:</td>
<td>DNS should work to resolve the host names.  (or /etc/hosts)</td>
</tr>
</tbody>
</table>

<p>The above can be accomplished in visualized environments.  Make sure your VM
environment has networking such that all of the hosts can connect to each other
on the same subnet.</p>

<h2 id="install-ceph-software">Install Ceph Software</h2>

<pre><code class="language-bash">&gt; sudo apt-get install ceph
</code></pre>

<p>Will install the Ceph components needed.  Perform this on all hosts.</p>

<h2 id="ceph-conf-configuration-file">ceph.conf Configuration file</h2>

<p>The following configuration file will be used on all hosts; it lives in
<code>/etc/ceph/ceph.conf</code>:</p>

<pre><code>fsid = d4b1b3b2-3bbb-4487-93ef-068d3ee73a22

mon_initial_members = a
mon_host = 10.3.7.91

auth_cluster_required = none
auth_service_required = none
auth_client_required = none
</code></pre>

<p>Two entries in this file should be changed:</p>

<ol>
<li><code>fsid = &lt;uuid&gt;         ## use 'uuidgen' to generate a unique UUID</code></li>
<li><code>mon_host = &lt;ip-addr&gt;  ## place the actual IP of 'ceph-mon0' here.</code></li>
</ol>

<h2 id="configure-the-monitor-host">Configure the Monitor host</h2>

<ul>
<li>Monitor host:  <code>ceph-mon0</code></li>
</ul>

<ol>
<li><p>Place the <code>ceph.conf</code> file from the previous step in /etc/ceph.  Owned by
root and world readable (0644).</p></li>

<li><p>Run <code>ceph-mon</code> to initialize the &ldquo;mon data&rdquo; directory (/var/lib/ceph/mon).
This monitor, the only monitor in this example, will be monitor &lsquo;a&rsquo;.  The <code>-i
a</code> flag identifies the monitor as having id &lsquo;a&rsquo;.  After running this command
there will be a <code>/var/lib/ceph/mon/ceph-a</code> directory.  The &lsquo;ceph&rsquo; corresponds
to the cluster name and the &lsquo;-a&rsquo; corresponds to monitor id &lsquo;a&rsquo;.</p></li>
</ol>

<pre><code class="language-bash">&gt; sudo ceph-mon --cluster=ceph -i a -f --mkfs
</code></pre>

<ol>
<li>Start <code>ceph-mon</code> as a daemon, (now that the data directory is initialized.</li>
</ol>

<pre><code>&gt; sudo ceph-mon --cluster=ceph -i a
</code></pre>

<ol>
<li>Verify the monitor is running (from ceph-mon0)</li>
</ol>

<pre><code class="language-bash">&gt; ceph health
HEALTH_ERR 64 pgs stuck inactive; 64 pgs stuck unclean; no osds
</code></pre>

<p>It is expected that the health is in error; there are no osd daemons running
yet.  If the call does not return, (hangs), that is the indicator of a problem.</p>

<h2 id="configure-and-join-each-osd-host">Configure and join each OSD host</h2>

<p>(optional:  Running <code>ceph -w</code> on host ceph-mon0 will allow you to &ldquo;tail&rdquo; the
activities of the cluster.  You should see output after running each <code>ceph-disk</code>
command.  The <code>ceph -w</code> command can actually be run from any host configured to
interact with the ceph cluster, but ceph-mon0 is convenient in this case.)</p>

<p>Repeat this process for each OSD host:  <code>mon-osd{0,1,2}</code></p>

<ol>
<li><p>Copy and install the <code>ceph.conf</code> file in /etc/ceph.  Owned by root and world
readable (0644).   [do not change the mon_host IP address.]</p></li>

<li><p>Ensure /dev/sdb is <em>UNMOUNTED</em> and available for both partitioning and
formatting.  <em>All data on /dev/sdb will be DESTROYED</em></p></li>
</ol>

<pre><code class="language-bash">&gt; sudo ceph-disk prepare \
    --cluster ceph \
    --cluster-uuid d4b1b3b2-3bbb-4487-93ef-068d3ee73a22 \
    --fs-type btrfs \
    /dev/sdb
Creating new GPT entries.
Setting name!
partNum is 1
REALLY setting name!
The operation has completed successfully.
Setting name!
partNum is 0
REALLY setting name!
The operation has completed successfully.
WARNING: --leafsize is deprecated, use --nodesize
btrfs-progs v4.0
See http://btrfs.wiki.kernel.org for more information.

Turning ON incompat feature 'extref': increased hardlink limit per file to 65536
Turning ON incompat feature 'skinny-metadata': reduced-size metadata extent refs
fs created label (null) on /dev/sdb1
     nodesize 32768 leafsize 32768 sectorsize 4096 size 15.00GiB
Warning: The kernel is still using the old partition table.
The new table will be used at the next reboot or after you
run partprobe(8) or kpartx(8)
The operation has completed successfully.

&gt; ceph df
GLOBAL:
    SIZE       AVAIL      RAW USED     %RAW USED
    15358M     15089M       33440k          0.21
POOLS:
    NAME     ID     USED     %USED     MAX AVAIL     OBJECTS
    rbd      0         0         0         5029M           0

&gt; ps -ef | grep ceph-osd
root    2248   1  0 19:08 ?   00:00:00 /usr/bin/ceph-osd --id 0 --foreground --cluster ceph -c /etc/ceph/ceph.conf
</code></pre>

<p>In the above example the initial size of /dev/sdb was 20G and preparing the disk
left approximately 155358M usable for storage.  The remainder went to the
partition used for the Ceph journal.</p>

<p>After applying the above command to the 2nd and 3rd (ceph-mon{1,2}) hosts you
will see an appropriate increase in the available storage.</p>

<p>Observing the output of <code>ps</code> you see that the <code>ceph-disk prepare</code> command has
started the OSD daemon as well.  Upon reboot <code>ceph-osd</code> will be automatically
started as a service.</p>

<p><em>Note</em>: <code>ceph-mon</code> is not configured to autostart using <em>this</em> set of
instructions.</p>

<p>Repeat for hosts mon-osd{1,2}</p>

<p>Done.</p>

<h1 id="exercise-the-ceph-cluster">Exercise the Ceph cluster</h1>

<p>The following commands will demonstrate basic cluster functionality; they can be
performed from any host that has:</p>

<ol>
<li>Direct network connectivity to the cluster (no NAT - in case you used
Virtualbox, et. al.)</li>
<li>The <code>ceph.conf</code> file from above placed in /etc/ceph and readable.</li>
<li>The <code>ceph</code> Ubuntu package installed (from step 1).</li>
</ol>

<p>A convenient, but not necessary, host to use are any of the hosts you just
installed.</p>

<h2 id="list-and-create-a-pool-for-storage">List and create a pool for storage</h2>

<p>Note:  the following commands do not require the use of <code>sudo</code>.  Authentication
would be acomplished via the cephx protocol if it had not been disabled in the
ceph.conf file, (the last three lines set the &ldquo;auth&rdquo; parameters to &lsquo;none&rsquo;).</p>

<pre><code class="language-bash">&gt; ceph osd pool ls
rbd
</code></pre>

<p>Ceph creates a default pool named &lsquo;rbd&rsquo; when initialized.</p>

<pre><code class="language-bash">&gt; rados mkpool data
successfully created pool data

&gt; rados df
pool name        KB  objects  clones  degraded  unfound   rd   rd KB   wr   wr KB
data              0        0       0         0        0    0       0    0       0
rbd               0        0       0         0        0    0       0    0       0
total used    102176       0
total avail 46354496
total space 47182788
</code></pre>

<h2 id="place-a-file-directly-in-the-data-pool">Place a file directly in the &lsquo;data&rsquo; pool</h2>

<pre><code class="language-bash">&gt; touch test-file.out
&gt; rados put -p data test-file.out test-file.out
&gt; rados ls -p data
test-file.out
</code></pre>

<p>This concludes the quintessential Ceph installation and simple demonstration that the cluster is functioning.</p>

<!--
Local Variables:
fill-column: 80
End:
-->

</main>

  <footer>
  
  <hr/>
  <h1 id="comments">Comments</h1>
  <div id="disqus_thread"></div>
<script>
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "egustafson" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


  
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-59717139-1', 'auto');
ga('send', 'pageview');
</script>


  
  <hr/>
  &copy; <a href="http://www.elfwerks.org/ericg">Eric Gustafson</a> 2017 | <a href="https://github.com/egustafson">Github</a> | <a href="https://twitter.com/elfwerks">Twitter</a>
  
  </footer>
  </body>
</html>

